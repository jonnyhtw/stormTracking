To use this for UM data:

1- Get the *a.pc* data, which should be 6 hourly.

2- Extract from tar archive if need be (the ampersand tells all the processes to be done at the same time)...

> for file in *.tar ; do echo $file ; tar -xvf $file --wildcards \*a.pc\*.pp & done

3- use mule-select to get field 16222 if necessary.

> for file in *a.pc*.pp ; do mule-select $file STASH-16222-$file --include lbuser4=16222 ; done

4- Move to /nesi/project/niwa00013/williamsjh/NZESM/storm/model-data/[suite ID] and rename them back to their original names...

> rename STASH-16222-[suite ID] [SUITE ID] STASH-16222*


5- Convert to NetCDF...

> module load Mule

>>> import iris
>>> import glob
>>> 
>>> files = sorted(glob.glob('./bm456a.pc*.pp'))# for example
>>> 
>>> for file in files:
>>>     print(file[:-3])
>>>     iris.save(iris.load_cube(file), file[:-3]+'.nc')

6- Regrid to NCEP/NCAR grid, for example...

for file in bb075a.pc*.nc ; do cdo remapbil,/nesi/project/niwa00013/williamsjh/NZESM/storm/data/NCEP/20CRv2c/prmsl/6hourly/prmsl.2014.nc $file regrid-$file ; done

7- Combine seasonal files, for example...

export suite=bm456
> for year in {1950..2014}; do echo $year ; cdo cat
> regrid-${suite}a.pc${year}0101.nc regrid-${suite}a.pc${year}0401.nc
> regrid-${suite}a.pc${year}0701.nc regrid-${suite}a.pc${year}1001.nc regrid-${suite}a.pc${year}.nc ; done

8- Delete .pp files in /nesi/project/niwa00013/williamsjh/NZESM/storm/model-data/[suite ID] and constituent, intermediate seasonal NetCDF files since they can be easily and quickly recreated.

9- Edit storm_detection.py if need be and run storm_detection.sl to parallelise the storm detection for each year.

10- Edit storm_tracking.py if need be and run storm_tracking.sl to parallelise the storm tracking for each year.
